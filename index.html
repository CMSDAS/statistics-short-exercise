<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="img/favicon.ico">

    
    <title>Home - CMSDasStats</title>
    

    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/v4-shims.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3.3.0/build/web/hack.min.css">
    <link href='//rsms.me/inter/inter.css' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,400,300,600,700&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href="css/bootstrap-custom.min.css" rel="stylesheet">
    <link href="css/base.min.css" rel="stylesheet">
    <link href="css/cinder.min.css" rel="stylesheet">

    
        
        <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/styles/github.min.css">
        
    
    <link href="mystyle.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
            <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
            <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
        <![endif]-->

    

     
</head>

<body class="homepage" >

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->

            <a class="navbar-brand" href=".">CMSDasStats</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">

            <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                            <i class="fas fa-search"></i> Search
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/CMSDAS/statistics-short-exercise/edit/master/docs/index.md"><i class="fab fa-github"></i> Edit on GitHub</a>
                    </li>
            </ul>
        </div>
    </div>
</div>

    <div class="container">
        
        
        <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="first-level active"><a href="#getting-started">Getting started</a></li>
        <li class="first-level "><a href="#roofit">RooFit</a></li>
            <li class="second-level"><a href="#variables">Variables</a></li>
                
            <li class="second-level"><a href="#functions-and-pdfs">Functions and PDFs</a></li>
                
            <li class="second-level"><a href="#plotting">Plotting</a></li>
                
            <li class="second-level"><a href="#workspaces">Workspaces</a></li>
                
        <li class="first-level "><a href="#combine">Combine</a></li>
        <li class="first-level "><a href="#combine-part-1-datacard-format-and-asymptotic-limits">Combine part 1: datacard format and asymptotic limits</a></li>
            <li class="second-level"><a href="#datacard-format">Datacard format</a></li>
                
            <li class="second-level"><a href="#asymptotic-limits">Asymptotic limits</a></li>
                
        <li class="first-level "><a href="#combine-part-2-shape-based-analysis">Combine part 2: shape-based analysis</a></li>
            <li class="second-level"><a href="#setting-up-the-datacard">Setting up the datacard</a></li>
                
            <li class="second-level"><a href="#running-combine-for-a-blind-analysis">Running combine for a blind analysis</a></li>
                
            <li class="second-level"><a href="#using-fitdiagnostics">Using FitDiagnostics</a></li>
                
            <li class="second-level"><a href="#mc-statistical-uncertainties">MC statistical uncertainties</a></li>
                
            <li class="second-level"><a href="#nuisance-parameter-impacts">Nuisance parameter impacts</a></li>
                
    </ul>
</div></div>
        <div class="col-md-9" role="main">

<h2 id="getting-started">Getting started</h2>
<p>We need to set up a new CMSSW area and checkout the combine package: </p>
<pre><code class="shell">export SCRAM_ARCH=slc7_amd64_gcc700
cmsrel CMSSW_10_2_13
cd CMSSW_10_2_13/src
cmsenv
git clone https://github.com/cms-analysis/HiggsAnalysis-CombinedLimit.git HiggsAnalysis/CombinedLimit
cd HiggsAnalysis/CombinedLimit

cd $CMSSW_BASE/src/HiggsAnalysis/CombinedLimit
git fetch origin
git checkout v8.1.0

cd $CMSSW_BASE/src
</code></pre>

<p>We will also make use another package, <code>CombineHarvester</code>, which contains some high-level tools for working with combine. The following command will download the repository and checkout just the parts of it we need for this tutorial:</p>
<pre><code class="shell">bash &lt;(curl -s https://raw.githubusercontent.com/cms-analysis/CombineHarvester/master/CombineTools/scripts/sparse-checkout-https.sh)
</code></pre>

<p>Now make sure the CMSSW area is compiled:</p>
<pre><code class="shell">scramv1 b clean; scramv1 b
</code></pre>

<p>Finally we will checkout the working directory for these tutorials - this contains all the inputs needed to run the exercises below:</p>
<pre><code class="shell">cd $CMSSW_BASE/src
git clone https://github.com/CMSDAS/statistics-short-exercise.git
cd statistics-short-exercise
</code></pre>

<h2 id="roofit">RooFit</h2>
<p>RooFit is an object-oriented analysis environment built on ROOT, with a collection of classes designed to augment ROOT for data modelling. Combine is in turn built on RooFit, so before learning about Combine, it is useful to get to grips with a few RooFit basics. We will do that in this section.</p>
<p>We will use python syntax in this section; you can either call the commands in an interactive python session, or just put them in a .py script. Make sure to do <code>from ROOT import *</code> at the top of your script (or in the interactive session)</p>
<h3 id="variables">Variables</h3>
<p>In RooFit, any variable, data point, function, PDF (etc.) is represented by a c++ object. The most basic of these is the RooRealVar. Let's create one which will represent the mass of some hypothetical particle, name it, and give a hypothetical starting value and range. </p>
<pre><code>MH = RooRealVar(&quot;MH&quot;,&quot;mass of the Hypothetical Boson (H-boson) in GeV&quot;,125,120,130)
MH.Print()
</code></pre>

<p>The output of this is</p>
<pre><code>RooRealVar::MH = 125  L(120 - 130)
</code></pre>

<p>So we now have a RooRealVar called MH, with default value 125 and range 120-130. We can now access the object and  change for example its value:</p>
<pre><code>MH.setVal(130)
print MH.getVal()
</code></pre>

<p>Which should print the new value, 130. </p>
<p>In particle detectors we typically don't observe this particle mass but usually define some observable which is sensitive to this mass. Lets assume we can detect and reconstruct the decay products of the H-boson and measure the invariant mass of those particles. We need to make another variable which represents that invariant mass. Make a RooRealVar "mass" with a default value of 100 GeV and a range 80-200</p>
<p>(solution to show with toggle:)</p>
<pre><code>mass = RooRealVar(&quot;m&quot;,&quot;m (GeV)&quot;,100,80,200)
</code></pre>

<p>In the perfect world we would perfectly measure the exact mass of the particle in every single event. However, our detectors are usually far from perfect so there will be some resolution effect. Lets assume the resolution of our measurement of the invariant mass is 10 GeV (range 0-20 GeV) and call it "sigma"</p>
<pre><code>sigma = RooRealVar(&quot;resolution&quot;,&quot;#sigma&quot;,10,0,20)
</code></pre>

<h3 id="functions-and-pdfs">Functions and PDFs</h3>
<p>More exotic variables can be constructed out of these <code>RooRealVar</code>s using <code>RooFormulaVar</code>s. For example, suppose we wanted to make a function out of the variables which represented the relative resolution as a function of the hypothetical mass MH.</p>
<pre><code>func = RooFormulaVar(&quot;R&quot;,&quot;@0/@1&quot;,RooArgList(sigma,mass))
func.Print(&quot;v&quot;)
</code></pre>

<p>The main objects we are interested in using from RooFit are probability density functions (PDFs). We can construct the PDF <span><span class="MathJax_Preview">f(m|M_H,\sigma)</span><script type="math/tex">f(m|M_H,\sigma)</script></span> as a Gaussian shape, a <code>RooGaussian</code>:</p>
<pre><code>gauss = RooGaussian(&quot;gauss&quot;,&quot;f(m|M_{H},#sigma)&quot;,mass,MH,sigma)
gauss.Print(&quot;v&quot;)
</code></pre>

<p>Notice how the gaussian PDF, like the <code>RooFormulaVar</code> depends on our <code>RooRealVar</code> objects, these are its servers. Its evaluation will depend on their values.</p>
<p>The main difference between PDFs and functions in RooFit is that PDFs are automatically normalised to unity, hence they represent a probability density, you don't need to normalise yourself. Let's plot it for the different values of <span><span class="MathJax_Preview">M_H</span><script type="math/tex">M_H</script></span>.</p>
<h3 id="plotting">Plotting</h3>
<p>First we need to make a canvas and a <code>RooPlot</code> object. This object needs to know what observable is going to be on the x-axis:</p>
<pre><code>can = TCanvas()
plot = mass.frame()
</code></pre>

<p>Now we can plot the gaussian PDF for several mass values. We set <span><span class="MathJax_Preview">M_H</span><script type="math/tex">M_H</script></span> to 130 GeV earlier on, so to plot this PDF for <span><span class="MathJax_Preview">M_H</span><script type="math/tex">M_H</script></span> of 130 GeV we just do</p>
<pre><code>gauss.plotOn(plot,RooFit.LineColor(kGreen+3))
</code></pre>

<p>Where we're using <code>kGreen+3</code> as the line colour. Notice that we need to tell RooFit on which <code>RooPlot</code> object we want to plot our PDF, even if we only have one such object.</p>
<p>Let's also plot the PDF with <span><span class="MathJax_Preview">M_H</span><script type="math/tex">M_H</script></span> at 120 GeV, in blue, and with <span><span class="MathJax_Preview">M_H</span><script type="math/tex">M_H</script></span> 125 GeV, in red:</p>
<pre><code>MH.setVal(120)
gauss.plotOn(plot,RooFit.LineColor(kBlue))

MH.setVal(125)
gauss.plotOn(plot,RooFit.LineColor(kRed))
</code></pre>

<p>Finally, let's try adding this PDF for <span><span class="MathJax_Preview">M_H</span><script type="math/tex">M_H</script></span> at 115 GeV in bright green. We'll use a dashed line for this one. Afterwards we'll draw the plot and save the canvas. </p>
<pre><code>MH.setVal(115)
gauss.plotOn(plot,RooFit.LineColor(kGreen),RooFit.LineStyle(2))

plot.Draw()
can.Draw()
can.SaveAs(&quot;gaussians.pdf&quot;)
</code></pre>

<p>Why do the blue and bright green lines overlap?</p>
<h3 id="workspaces">Workspaces</h3>
<p>Before we move on to Combine, we'll look at how to store RooFit objects and links between them. We can do this with a <code>RooWorkspace</code>. Let's create one and import our PDF, and the <code>RooFormulaVar</code> we created earlier:</p>
<pre><code>w = RooWorkspace(&quot;w&quot;)

getattr(w,'import')(gauss)
getattr(w,'import')(func)

w.writeToFile(&quot;workspace.root&quot;)
</code></pre>

<p>Notice that the <code>RooRealVar</code>s we created are also getting imported into the workspace as our gaussian PDF depends on all three of them.</p>
<p>Now we can open the file that we've created in root:</p>
<pre><code>root workspace.root
.ls
</code></pre>

<details>
<summary><b>Show output </b></summary>

You should see that our workspace, named `w` is in the file:


<pre><code>TFile**     workspace.root
 TFile*     workspace.root
  KEY: RooWorkspace w;1 w
  KEY: TProcessID   ProcessID0;1    94b05638-d0c4-11ea-a5b3-84978a89beef
</code></pre>


</details>

<p>We can inspect its contents:</p>
<pre><code>w-&gt;Print()
</code></pre>

<details>
<summary><b>Show output </b></summary>

<pre><code>RooWorkspace(w) w contents

variables
---------
(MH,m,resolution)

p.d.f.s
-------
RooGaussian::gauss[ x=m mean=MH sigma=resolution ] = 0.135335

functions
--------
RooFormulaVar::R[ actualVars=(resolution,m) formula=&quot;@0/@1&quot; ] = 0.1
</code></pre>

</details>

<pre><code>
</code></pre>
<p>Now we can check the properties of some of the objects, for example:</p>
<pre><code>w-&gt;pdf(&quot;gauss&quot;)-&gt;Print(&quot;v&quot;)
</code></pre>

<details>
<summary><b>Show output </b></summary>



<pre><code>--- RooAbsArg ---
  Value State: DIRTY
  Shape State: DIRTY
  Attributes:
  Address: 0x63b62e0
  Clients:
  Servers:
    (0x6781400,V-) RooRealVar::m &quot;m (GeV)&quot;
    (0x687edd0,V-) RooRealVar::MH &quot;mass of the Hypothetical Boson (H-boson) in GeV&quot;
    (0x6795740,V-) RooRealVar::resolution &quot;#sigma&quot;
  Proxies:
    x -&gt; m
    mean -&gt; MH
    sigma -&gt; resolution
--- RooAbsReal ---

  Plot label is &quot;gauss&quot;
--- RooAbsPdf ---
Cached value = 0
</code></pre>


</details>

<pre><code>
</code></pre>
<p>We can also check and change the values of our <code>RooRealVar</code>s</p>
<pre><code>w-&gt;var(&quot;MH&quot;)-&gt;Print()
w-&gt;var(&quot;MH&quot;)-&gt;setVal(123)
w-&gt;var(&quot;MH&quot;)-&gt;getVal()
</code></pre>

<p>Gives:</p>
<pre><code>RooRealVar::MH = 120  L(120 - 130)
</code></pre>

<p>and</p>
<pre><code>(double) 123.00000
</code></pre>

<p>Note that if you close the file containing the workspace, open it again and call
<code>w-&gt;var("MH")-&gt;getVal()</code></p>
<p>You will get the value as set when the workspace was created again, in our case that's 120 GeV:</p>
<pre><code>(double) 120.00000
</code></pre>

<h2 id="combine">Combine</h2>
<p>For this course we will work with a simplified version of a real analysis, that nonetheless will have many features of the full analysis. The analysis is a search for an additional heavy neutral Higgs boson decaying to tau lepton pairs. Such a signature is predicted in many extensions of the standard model, in particular the minimal supersymmetric standard model (MSSM). You can read about the analysis in the paper <a href="https://arxiv.org/pdf/1803.06553.pdf">here</a>. The statistical inference makes use of a variable called the total transverse mass (<span><span class="MathJax_Preview">M_{\mathrm{T}}^{\mathrm{tot}}</span><script type="math/tex">M_{\mathrm{T}}^{\mathrm{tot}}</script></span>) that provides good discrimination between the resonant high-mass signal and the main backgrounds, which have a falling distribution in this high-mass region. The events selected in the analysis are split into a several categories which target the main di-tau final states as well as the two main production modes: gluon-fusion (ggH) and b-jet associated production (bbH). One example is given below for the fully-hadronic final state in the b-tag category which targets the bbH signal:</p>
<p><img alt="" src="images/CMS-DAS.003.jpeg" /></p>
<p>Initially we will start with the simplest analysis possible: a one-bin counting experiment using just the high <span><span class="MathJax_Preview">M_{\mathrm{T}}^{\mathrm{tot}}</span><script type="math/tex">M_{\mathrm{T}}^{\mathrm{tot}}</script></span> region of this distribution, and we will expand on this by turning this into a shape-based analysis. </p>
<h2 id="combine-part-1-datacard-format-and-asymptotic-limits">Combine part 1: datacard format and asymptotic limits</h2>
<h3 id="datacard-format">Datacard format</h3>
<p>We will begin with a simplified version of a datacard from the MSSM <span><span class="MathJax_Preview">\phi\rightarrow\tau\tau</span><script type="math/tex">\phi\rightarrow\tau\tau</script></span> analysis that has been converted to a one-bin counting experiment, as described above. While the full analysis considers a range of signal mass hypotheses, we will start by considering just one: <span><span class="MathJax_Preview">m_{\phi}</span><script type="math/tex">m_{\phi}</script></span>=800GeV. Click the text below to study the datacard (<code>datacard_part1.txt</code> in the <code>cms-das-stats-2020</code> directory):</p>
<details>
<summary><b>Show datacard</b></summary>

<pre><code class="shell">imax    1 number of bins
jmax    4 number of processes minus 1
kmax    * number of nuisance parameters
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
bin          signal_region
observation  10.0
--------------------------------------------------------------------------------
bin                      signal_region   signal_region   signal_region   signal_region   signal_region
process                  ttbar           diboson         Ztautau         jetFakes        bbHtautau
process                  1               2               3               4               0
rate                     4.43803         3.18309         3.7804          1.63396         0.711064
--------------------------------------------------------------------------------
CMS_eff_b          lnN   1.02            1.02            1.02            -               1.02
CMS_eff_t          lnN   1.12            1.12            1.12            -               1.12
CMS_eff_t_highpt   lnN   1.1             1.1             1.1             -               1.1
acceptance_Ztautau lnN   -               -               1.08            -               -
acceptance_bbH     lnN   -               -               -               -               1.05
acceptance_ttbar   lnN   1.005           -               -               -               -
lumi_13TeV         lnN   1.025           1.025           1.025           -               1.025
norm_jetFakes      lnN   -               -               -               1.2             -
xsec_Ztautau       lnN   -               -               1.04            -               -
xsec_diboson       lnN   -               1.05            -               -               -
xsec_ttbar         lnN   1.06            -               -               -               -
</code></pre>


</details>

<p>The layout of the datacard is as follows:</p>
<ul>
<li>At the top are the numbers <code>imax</code>, <code>jmax</code> and <code>kmax</code> representing the number of bins, processes and nuisance parameters respectively. Here a "bin" can refer to a literal single event count as in this example, or a full distribution we are fitting, in general with many histogram bins, as we will see later. We will refer to both as "channels" from now on. It is possible to replace these numbers with <code>*</code> and they will be deduced automatically.</li>
<li>The first line starting with <code>bin</code> gives a unique label to each channel, and the following line starting with <code>observation</code> gives the number of events observed in data.</li>
<li>In the remaining part of the card there are several columns: each one represents one process in one channel. The first four lines labelled <code>bin</code>, <code>process</code>, <code>process</code> and <code>rate</code> give the channel label, the process label, a process identifier (<code>&lt;=0</code> for signal, <code>&gt;0</code> for background) and the number of expected events respectively.</li>
<li>The remaining lines describe sources of systematic uncertainty. Each line gives the name of the uncertainty, (which will become the name of the nuisance parameter inside our RooFit model), the type of uncertainty ("lnN" = log-normal normalisation uncertainty) and the effect on each process in each channel. E.g. a 20% uncertainty on the yield is written as 1.20.</li>
<li>It is also possible to add a hash symbol (<code>#</code>) at the start of a line, which combine will then ignore when it reads the card.</li>
</ul>
<p>We can now run combine directly using this datacard as input. The general format for running combine is:</p>
<pre><code class="shell">combine -M [method] [datacard] [additional options...]
</code></pre>

<h3 id="asymptotic-limits">Asymptotic limits</h3>
<p>As we are searching for a signal process that does not exist in the standard model, it's natural to set an upper limit on the cross section times branching fraction of the process (assuming our dataset does not contain a significant discovery of new physics). Combine has dedicated method for calculating upper limits. The most commonly used one is <code>AsymptoticLimits</code>, which implements the CLs criterion and uses the profile likelihood ratio as the test statistic. As the name implies, the test statistic distributions are determined analytically in the asymptotic approximation, so there is no need for more time-intensive toy throwing and fitting. Try running the following command:</p>
<pre><code class="shell">combine -M AsymptoticLimits datacard_part1.txt -n .part1A
</code></pre>

<p>You should see the results of the observed and expected limit calculations printed to the screen. Here we have added an extra option, <code>-n .part1A</code>, which is short for <code>--name</code>, and is used to label the output file combine produces, which in this case will be called <code>higgsCombine.part1A.AsymptoticLimits.mH120.root</code>. The file name depends on the options we ran with, and is of the form: <code>higgsCombine[name].[method].mH[mass].root</code>. The file contains a TTree called <code>limit</code> which stores the numerical values returned by the limit computation. Note that in our case we did not set a signal mass when running combine (i.e. <code>-m 800</code>), so the output file just uses the default value of <code>120</code>. This does not affect our result in any way though, just the label that is used on the output file.</p>
<p>The limits are given on a parameter called <code>r</code>. This is the default <strong>parameter of interest (POI)</strong> that is added to the model automatically. It is a linear scaling of the normalisation of all signal processes given in the datacard, i.e. if <span><span class="MathJax_Preview">s_{i,j}</span><script type="math/tex">s_{i,j}</script></span> is the nominal number of signal events in channel <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> for signal process <span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>, then the normalisation of that signal in the model is given as <span><span class="MathJax_Preview">r\cdot s_{i,j}(\vec{\theta})</span><script type="math/tex">r\cdot s_{i,j}(\vec{\theta})</script></span>, where <span><span class="MathJax_Preview">\vec{\theta}</span><script type="math/tex">\vec{\theta}</script></span> represents the set of nuisance parameters which may also affect the signal normalisation. We therefore have some choice in the interpretation of r: for the measurement of a process with a well defined SM prediction we may enter this as the nominal yield in the datacard, such that <span><span class="MathJax_Preview">r=1</span><script type="math/tex">r=1</script></span> corresponds to this SM expectation, whereas for setting limits on BSM processes we may choose the nominal yield to correspond to some cross section, e.g. 1 pb, such that we can interpret the limit as a cross section limit directly. In this example the signal has been normalised to a cross section times branching fraction of 1 fb.</p>
<p>The expected limit is given under the background-only hypothesis. The median value under this hypothesis as well as the quantiles needed to give the 68% and 95% intervals are also calculated. These are all the ingredients needed to produce the standard limit plots you will see in many CMS results, for example the <span><span class="MathJax_Preview">\sigma \times \mathcal{B}</span><script type="math/tex">\sigma \times \mathcal{B}</script></span> limits for the <span><span class="MathJax_Preview">\text{bb}\phi\rightarrow\tau\tau</span><script type="math/tex">\text{bb}\phi\rightarrow\tau\tau</script></span> process:</p>
<p><img alt="" src="images/CMS-DAS.002.jpeg" /></p>
<p>In this case we only computed the values for one signal mass hypothesis, indicated by a red dashed line.</p>
<p><strong>Tasks and questions:</strong></p>
<ul>
<li>There are some important uncertainties missing from the datacard above. Add the uncertainty on the luminosity (name: <code>lumi_13TeV</code>) which has a 2.5% effect on all processes (except the <code>jetFakes</code>, which are taken from data), and uncertainties on the inclusive cross sections of the <code>Ztautau</code> and <code>ttbar</code> processes (with names <code>xsec_Ztautau</code> and <code>xsec_diboson</code>) which are 4% and 6% respectively.</li>
<li>Try changing the values of some uncertainties (up or down, or removing them altogether) - how do the expected and observed limits change?</li>
<li>Now try changing the number of observed events. The observed limit will naturally change, but the expected does too - why might this be?</li>
</ul>
<p>There are other command line options we can supply to combine which will change its behaviour when run. You can see the full set of supported options by doing <code>combine -h</code>. Many options are specific to a given method, but others are more general and are applicable to all methods. Throughout this tutorial we will highlight some of the most useful options you may need to use, for example:</p>
<ul>
<li>The range on the signal strength modifier: <code>--rMin=X</code> and <code>--rMax=Y</code>: In RooFit parameters can optionally have a range specified. The implication of this is that their values cannot be adjusted beyond the limits of this range. The min and max values can be adjusted though, and we might need to do this for our POI <code>r</code> if the order of magnitude of our measurement is different from the default range of <code>[0, 20]</code>. This will be discussed again later in the tutorial.</li>
<li>Verbosity: <code>-v X</code>: By default combine does not usually produce much output on the screen other the main result at the end. However, much more detailed information can be printed by setting the <code>-v N</code> with N larger than zero. For example at <code>-v 3</code> the logs from the minimizer, Minuit, will also be printed. These are very useful for debugging problems with the fit.</li>
</ul>
<h2 id="combine-part-2-shape-based-analysis">Combine part 2: shape-based analysis</h2>
<h3 id="setting-up-the-datacard">Setting up the datacard</h3>
<p>Now we move to the next step: instead of a one-bin counting experiment we will fit a binned distribution. In a typical analysis we will produce TH1 histograms of some variable sensitive to the presence of signal: one for the data and one for each signal and background processes. Then we add a few extra lines to the datacard to link the declared processes to these shapes which are saved in a ROOT file, for example:</p>
<details>
<summary><b>Show datacard</b></summary>

<pre><code class="shell">imax 1
jmax 1
kmax *
---------------
shapes * * simple-shapes-TH1_input.root $PROCESS $PROCESS_$SYSTEMATIC
shapes signal * simple-shapes-TH1_input.root $PROCESS$MASS $PROCESS$MASS_$SYSTEMATIC
---------------
bin bin1
observation 85
------------------------------
bin             bin1       bin1
process         signal     background
process         0          1
rate            10         100
--------------------------------
lumi     lnN    1.10       1.0
bgnorm   lnN    1.00       1.3
alpha  shape    -          1
</code></pre>

</details>

<p>Note that as with the one-bin card, the total nominal rate of a given process must be specified in the <code>rate</code> line of the datacard. This should agree with the value returned by <code>TH1::Integral</code>. However, we can also put a value of <code>-1</code> and the Integral value will be substituted automatically.</p>
<p>There are two other differences with respect to the one-bin card:</p>
<ul>
<li>A new block of lines at the top defining how channels and processes are mapped to the histograms (more than one line can be used)</li>
<li>In the list of systematic uncertainties some are marked as shape instead of lnN</li>
</ul>
<p>The syntax of the "shapes" line is: <code>shapes [process] [channel] [file] [histogram] [histogram_with_systematics]</code>. It is possible to use the <code>*</code> wildcard to map multiple processes and/or channels with one line. The histogram entries can contain the <code>$PROCESS</code>, <code>$CHANNEL</code> and <code>$MASS</code> place-holders which will be substituted when searching for a given (process, channel) combination. The value of <code>$MASS</code> is specified by the <code>-m</code> argument when combine. By default the observed data process name will be <code>data_obs</code>.</p>
<p>Shape uncertainties can be added by supplying two additional histograms for a process, corresponding to the distribution obtained by shifting that parameter up and down by one standard deviation. These shapes will be interpolated quadratically for shifts below <span><span class="MathJax_Preview">1\sigma</span><script type="math/tex">1\sigma</script></span> and linearly beyond. The normalizations are interpolated linearly in log scale just like we do for log-normal uncertainties.</p>
<p>The final argument of the "shapes" line above should contain the <code>$SYSTEMATIC</code> place-holder which will be substituted by the systematic name given in the datacard.</p>
<p>In the list of uncertainties the interpretation of the values for <code>shape</code> lines is a bit different from <code>lnN</code>. The effect can be "-" or 0 for no effect, 1 for normal effect, and possibly something different from 1 to test larger or smaller effects (in that case, the unit Gaussian is scaled by that factor before using it as parameter for the interpolation).</p>
<p>In this section we will use a datacard corresponding to the full distribution that was shown at the start of section 1, not just the high mass region. Have a look at <code>datacard_part2.txt</code>: this is still currently a one-bin counting experiment, however the yields are much higher since we now consider the full range of <span><span class="MathJax_Preview">M_{\mathrm{T}}^{\mathrm{tot}}</span><script type="math/tex">M_{\mathrm{T}}^{\mathrm{tot}}</script></span>. If you run the asymptotic limit calculation on this you should find the sensitivity is significantly worse than before.</p>
<p>The <strong>first task</strong> is to convert this to a shape analysis: the file <code>datacard_part2.shapes.root</code> contains all the necessary histograms, including those for the relevant shape systematic uncertainties. Add the relevant <code>shapes</code> lines to the top of the datacard (after the <code>kmax</code> line) to map the processes to the correct TH1s in this file. Hint: you will need a different line for the signal process.</p>
<p>Compared to the counting experiment we must also consider the effect of uncertainties that change the shape of the distribution. Some, like <code>CMS_eff_t_highpt</code>, were present before, as it has both a shape and normalisation effect. Others are primarily shape effects so were not included before.</p>
<p>Add the following shape uncertainties: <code>top_pt_ttbar_shape</code> affecting <code>ttbar</code>,the tau energy scale uncertainties <code>CMS_scale_t_1prong0pi0_13TeV</code>, <code>CMS_scale_t_1prong1pi0_13TeV</code> and <code>CMS_scale_t_3prong0pi0_13TeV</code> affecting all processes except <code>jetFakes</code>, and <code>CMS_eff_t_highpt</code> also affecting the same processes.</p>
<p>Once this is done you can run the asymptotic limit calculation on this datacard. From now on we will convert the text datacard into a RooFit workspace ourselves instead of combine doing it internally every time we run. This is a good idea for more complex analyses since the conversion step can take a notable amount of time. For this we use the <code>text2workspace.py</code> command:</p>
<pre><code class="shell">text2workspace.py datacard_part2.txt -m 800 -o workspace_part2.root
</code></pre>

<p>And then we can use this as input to combine instead of the text datacard:</p>
<pre><code class="shell">combine -M AsymptoticLimits workspace_part2.root -m 800
</code></pre>

<p><strong>Tasks and questions:</strong></p>
<ul>
<li>Verify that the sensitivity of the shape analysis is indeed improved over the counting analysis in the first part.</li>
</ul>
<h3 id="running-combine-for-a-blind-analysis">Running combine for a blind analysis</h3>
<p>Most analyses are developed and optimised while we are "blind" to the region of data where we expect our signal to be. With <code>AsymptoticLimits</code> we can choose just to run the expected limit (<code>--run expected</code>), so as not to calculate the observed. However the data is still used, even for the expected, since in the frequentist approach a background-only fit to the data is performed to define the Asimov dataset used to calculate the expected limits. To skip this fit to data and use the pre-fit state of the model the option <code>--run blind</code> or <code>--noFitAsimov</code> can be used. <strong>Task:</strong> Compare the expected limits calculated with <code>--run expected</code> and <code>--run blind</code>. Why are they different?</p>
<p>A more general way of blinding is to use combine's toy and Asimov dataset generating functionality. You can read more about this <a href="http://cms-analysis.github.io/HiggsAnalysis-CombinedLimit/part3/runningthetool/#toy-data-generation">here</a>. These options can be used with any method in combine, not just <code>AsymptoticLimits</code>.</p>
<p><strong>Task:</strong> Calculate a blind limit by generating a background-only Asimov with the <code>-t -1</code> option instead of using the <code>AsymptoticLimits</code> specific options. You should find the observed limit is the same as the expected. Then see what happens if you inject a signal into the Asimov dataset using the <code>--expectSignal [X]</code> option.</p>
<h3 id="using-fitdiagnostics">Using FitDiagnostics</h3>
<p>We will now explore one of the most commonly used modes of combine: <code>FitDiagnostics</code> . As well as allowing us to make a <strong>measurement</strong> of some physical quantity (as opposed to just setting a limit on it), this method is useful to gain additional information about the model and the behaviour of the fit. It performs two fits:</p>
<ul>
<li>A "background-only" (b-only) fit: first POI (usually "r") fixed to zero</li>
<li>A "signal+background" (s+b) fit: all POIs are floating</li>
</ul>
<p>With the s+b fit combine will report the best-fit value of our signal strength modifier <code>r</code>. As well as the usual output file, a file named <code>fitdiagnostics.root</code> is produced which contains additional information. In particular it includes two <code>RooFitResult</code> objects, one for the b-only and one for the s+b fit, which store the fitted values of all the <strong>nuisance parameters (NPs)</strong> and POIs as well as estimates of their uncertainties. The covariance matrix from both fits is also included, from which we can learn about the correlations between parameters. Run the <code>FitDiagnostics</code> method on our workspace:</p>
<pre><code class="shell">combine -M FitDiagnostics workspace_part2.root -m 800 --rMin -20 --rMax 20
</code></pre>

<p>Open the resulting <code>fitDiagnostics.root</code> interactively and print the contents of the s+b RooFitResult:</p>
<pre><code class="shell">root [1] fit_s-&gt;Print()
</code></pre>

<details>
<summary><b>Show output</b></summary>

<pre><code class="shell"> RooFitResult: minimized FCN value: -4.7666, estimated distance to minimum: 3.31389e-05
                covariance matrix quality: Full, accurate covariance matrix
                Status : MINIMIZE=0 HESSE=0

   Floating Parameter    FinalValue +/-  Error
--------------------  --------------------------
             CMS_eff_b   -4.3559e-02 +/-  9.87e-01
             CMS_eff_t   -2.6382e-01 +/-  7.27e-01
      CMS_eff_t_highpt   -4.7214e-01 +/-  9.56e-01
      CMS_scale_t_1prong0pi0_13TeV   -1.5884e-01 +/-  5.89e-01
      CMS_scale_t_1prong1pi0_13TeV   -1.6512e-01 +/-  4.91e-01
      CMS_scale_t_3prong0pi0_13TeV   -3.0668e-01 +/-  6.03e-01
    acceptance_Ztautau   -3.1059e-01 +/-  8.57e-01
        acceptance_bbH   -5.8325e-04 +/-  9.94e-01
      acceptance_ttbar    4.7839e-03 +/-  9.94e-01
            lumi_13TeV   -5.4684e-02 +/-  9.83e-01
         norm_jetFakes   -9.3975e-02 +/-  2.54e-01
                     r   -2.7327e+00 +/-  2.57e+00
    top_pt_ttbar_shape    1.7614e-01 +/-  6.97e-01
          xsec_Ztautau   -1.5991e-01 +/-  9.61e-01
          xsec_diboson    3.8745e-02 +/-  9.94e-01
            xsec_ttbar    5.8025e-02 +/-  9.41e-01
</code></pre>

</details>

<p>There are several useful pieces of information here. At the top the status codes from the fits that were performed is given. In this case we can see that two algorithms were run: <code>MINIMIZE</code> and <code>HESSE</code>, both of which returned a successful status code (0). Both of these are routines in the <strong>Minuit2</strong> minimization package - the default minimizer used in RooFit. The first performs the main fit to the data, and the second calculates the covariance matrix at the best-fit point. It is important to always check this second step was successful and the message "Full, accurate covariance matrix" is printed, otherwise the parameter uncertainties can be very inaccurate, even if the fit itself was successful.</p>
<p>Underneath this the best-fit values (<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>) and symmetrised uncertainties for all the floating parameters are given. For all the constrained nuisance parameters a convention is used by which the nominal value (<span><span class="MathJax_Preview">\theta_I</span><script type="math/tex">\theta_I</script></span>) is zero, corresponding to the mean of a Gaussian constraint PDF with width 1.0, such that the parameter values <span><span class="MathJax_Preview">\pm 1.0</span><script type="math/tex">\pm 1.0</script></span> correspond to the <span><span class="MathJax_Preview">\pm 1\sigma</span><script type="math/tex">\pm 1\sigma</script></span> input uncertainties.</p>
<p>A more useful way of looking at this is to compare the pre- and post-fit values of the parameters, to see how much the fit to data has shifted and constrained these parameters with respect to the input uncertainty. The script <code>diffNuisances.py</code> can be used for this:</p>
<pre><code class="shell">python diffNuisances.py fitDiagnostics.root --all
</code></pre>

<details>
<summary><b>Show output</b></summary>

<pre><code class="shell">name                                              b-only fit            s+b fit         rho
CMS_eff_b                                        -0.04, 0.99        -0.04, 0.99       +0.01
CMS_eff_t                                     * -0.24, 0.73*     * -0.26, 0.73*       +0.06
CMS_eff_t_highpt                              * -0.56, 0.93*     * -0.47, 0.96*       +0.03
CMS_scale_t_1prong0pi0_13TeV                  * -0.17, 0.58*     * -0.16, 0.59*       -0.04
CMS_scale_t_1prong1pi0_13TeV                  ! -0.12, 0.45!     ! -0.17, 0.49!       +0.21
CMS_scale_t_3prong0pi0_13TeV                  * -0.31, 0.60*     * -0.31, 0.60*       +0.02
acceptance_Ztautau                            * -0.31, 0.86*     * -0.31, 0.86*       -0.05
acceptance_bbH                                   +0.00, 0.99        -0.00, 0.99       +0.05
acceptance_ttbar                                 +0.01, 0.99        +0.00, 0.99       +0.00
lumi_13TeV                                       -0.05, 0.98        -0.05, 0.98       +0.01
norm_jetFakes                                 ! -0.09, 0.25!     ! -0.09, 0.25!       -0.05
top_pt_ttbar_shape                            * +0.24, 0.69*     * +0.18, 0.70*       +0.23
xsec_Ztautau                                     -0.16, 0.96        -0.16, 0.96       -0.02
xsec_diboson                                     +0.03, 0.99        +0.04, 0.99       -0.02
xsec_ttbar                                       +0.08, 0.94        +0.06, 0.94       +0.02
</code></pre>


</details>

<p>The numbers in each column are respectively <span><span class="MathJax_Preview">\frac{\theta-\theta_I}{\sigma_I}</span><script type="math/tex">\frac{\theta-\theta_I}{\sigma_I}</script></span> (often called the <strong>pull</strong>, though note that more than one definition is in use for this), where <span><span class="MathJax_Preview">\sigma_I</span><script type="math/tex">\sigma_I</script></span> is the input uncertainty; and the ratio of the post-fit to the pre-fit uncertainty <span><span class="MathJax_Preview">\frac{\sigma}{\sigma_I}</span><script type="math/tex">\frac{\sigma}{\sigma_I}</script></span>.</p>
<p><strong>Tasks and questions:</strong></p>
<ul>
<li>Which parameter has the largest pull? Which has the tightest constraint?</li>
<li>Should we be concerned when a parameter is more strongly constrained than the input uncertainty (i.e. <span><span class="MathJax_Preview">\frac{\sigma}{\sigma_I}&lt;1.0</span><script type="math/tex">\frac{\sigma}{\sigma_I}<1.0</script></span>)?</li>
<li>Check the pulls and constraints on a b-only and s+b asimov dataset instead. This check is <a href="https://twiki.cern.ch/twiki/bin/viewauth/CMS/HiggsWG/HiggsPAGPreapprovalChecks">required</a> for all analyses in the Higgs PAG. It serves both as a closure test (do we fit exactly what signal strength we input?) and a way to check whether there are any infeasibly strong constraints while the analysis is still blind (typical example: something has probably gone wrong if we constrain the luminosity uncertainty to 10% of the input!)</li>
</ul>
<h3 id="mc-statistical-uncertainties">MC statistical uncertainties</h3>
<p>So far there is an important source of uncertainty we have neglected. Our estimates of the backgrounds come either from MC simulation or from sideband regions in data, and in both cases these estimates are subject to a statistical uncertainty on the number of simulated or data events. 
In principle we should include an independent statistical uncertainty for every bin of every process in our model. 
It's important to note that combine/RooFit does not take this into account automatically - statistical fluctuations of the data are implicitly accounted 
for in the likelihood formalism, but statistical uncertainties in the model must be specified by us.</p>
<p>One way to implement these uncertainties is to create a <code>shape</code> uncertainty for each bin of each process, in which the up and down histograms have the contents of the bin
 shifted up and down by the <span><span class="MathJax_Preview">1\sigma</span><script type="math/tex">1\sigma</script></span> uncertainty. 
However this makes the likelihood evaluation computationally inefficient, and can lead to a large number of nuisance parameters 
in more complex models. Instead we will use a feature in combine called <code>autoMCStats</code> that creates these automatically from the datacard, 
and uses a technique called "Barlow-Beeston-lite" to reduce the number of systematic uncertainties that are created. 
This works on the assumption that for high MC event counts we can model the uncertainty with a Gaussian distribution. Given the uncertainties in different bins are independent, the total uncertainty of several processes in a particular bin is just the sum of <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> individual Gaussians, which is itself a Gaussian distribution. 
So instead of <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> nuisance parameters we need only one. This breaks down when the number of events is small and we are not in the Gaussian regime. 
The <code>autoMCStats</code> tool has a threshold setting on the number of events below which the the Barlow-Beeston-lite approach is not used, and instead a 
Poisson PDF is used to model per-process uncertainties in that bin.</p>
<p>After reading the full documentation on <code>autoMCStats</code> <a href="http://cms-analysis.github.io/HiggsAnalysis-CombinedLimit/part2/bin-wise-stats/">here</a>, add the corresponding line to your datacard. 
Start by setting a threshold of 0, i.e. <code>[channel] autoMCStats 0</code>, to force the use of Barlow-Beeston-lite in all bins.</p>
<p><strong>Tasks and questions:</strong></p>
<ul>
<li>Check how much the cross section measurement and uncertainties change using <code>FitDiagnostics</code>.</li>
<li>It is also useful to check how the expected uncertainty changes using an Asimov dataset, say with <code>r=10</code> injected.</li>
<li><strong>Advanced task:</strong> See what happens if the Poisson threshold is increased. Based on your results, what threshold would you recommend for this analysis?</li>
</ul>
<h3 id="nuisance-parameter-impacts">Nuisance parameter impacts</h3>
<p>For this section we're going to use a different datacard - the only reason for this is that what we're going to do next is more informative for a lower mass point (at 200 GeV). So we will first make a new workspace:</p>
<pre><code class="shell">text2workspace.py datacard_part2b.txt -m 200 -o workspace_part2b.root
</code></pre>

<p>It is often useful to examine in detail the effects the systematic uncertainties have on the signal strength measurement. This is often referred to as calculating the "impact" of each uncertainty. What this means is to determine the shift in the signal strength, with respect to the best-fit, that is induced if a given nuisance parameter is shifted by its <span><span class="MathJax_Preview">\pm1\sigma</span><script type="math/tex">\pm1\sigma</script></span> post-fit uncertainty values. If the signal strength shifts a lot, it tells us that it has a strong dependency on this systematic uncertainty. In fact, what we are measuring here is strongly related to the correlation coefficient between the signal strength and the nuisance parameter. The <code>MultiDimFit</code> method has an algorithm for calculating the impact for a given systematic: <code>--algo impact -P [parameter name]</code>, but it is typical to use a higher-level script, <code>combineTool.py</code> (part of the CombineHarvester package you checked out at the beginning) to automatically run the impacts for all parameters. Full documentation on this is given <a href="http://cms-analysis.github.io/HiggsAnalysis-CombinedLimit/part3/nonstandard/#nuisance-parameter-impacts">here</a>. There is a three step process for running this. First we perform an initial fit for the signal strength and its uncertainty:</p>
<pre><code class="shell">combineTool.py -M Impacts -d workspace_part2b.root -m 200 --rMin -1 --rMax 2 --robustFit 1 --doInitialFit
</code></pre>

<p>Then we run the impacts for all the nuisance parameters:</p>
<pre><code class="shell">combineTool.py -M Impacts -d workspace_part2b.root -m 200 --rMin -1 --rMax 2 --robustFit 1 --doFits
</code></pre>

<p>This will take a little bit of time. When finished we collect all the output and convert it to a json file:</p>
<pre><code class="shell">combineTool.py -M Impacts -d workspace_part2b.root -m 200 --rMin -1 --rMax 2 --robustFit 1 --output impacts.json
</code></pre>

<p>We can then make a plot showing the pulls and parameter impacts, sorted by the largest impact:</p>
<pre><code class="shell">plotImpacts.py -i impacts.json -o impacts
</code></pre>

<p><strong>Tasks and questions:</strong></p>
<ul>
<li>Identify the most important uncertainties using the impacts tool.</li>
</ul></div>
        
        
    </div>

    <footer class="col-md-12 text-center">
        
        <hr>
        <p>
        <small>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</small>
        </p>

        
        
    </footer>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="js/bootstrap-3.0.3.min.js"></script>
    
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.0/build/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    
    <script>var base_url = "."</script>
    
    <script src="js/base.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script src="search/main.js"></script>

    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal">
                    <span aria-hidden="true">&times;</span>
                    <span class="sr-only">Close</span>
                </button>
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>
    </body>

</html>

<!--
MkDocs version : 1.1
Build Date UTC : 2020-08-18 13:38:25
-->
